{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Logistic Regression\n",
    "\n",
    "In the introduction, we touched on what logistic regression is and what a model looks like. Now, the idea needs to be extended to multiple input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All good python projects begin with specifying which modules to load\n",
    "\n",
    "import pandas as pd  # Pandas is a package which creates data frames\n",
    "import numpy as np # Numpy is the package which creates/manages/operates on numerical data\n",
    "import matplotlib.pyplot as plt # Matplotlib is the plotting library\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Every project begins with the data.  We will be using data that _Tjen-Sien Lim_ (limt@stat.wisc.edu) supplied. The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\n",
    "\n",
    "//=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "Dataset:  haberman.data\n",
    "Lim, Tjen-Sien (1999). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "Variables/Columns\n",
    "\n",
    "1. Age of patient at time of operation (numerical) <br>\n",
    "2. Patient's year of operation (year - 1900, numerical)<br> \n",
    "3. Number of positive axillary nodes detected (numerical) <br>\n",
    "4. Survival status (class attribute) <br>\n",
    "-- 1 = the patient survived 5 years or longer <br>\n",
    "-- 2 = the patient died within 5 year <br>\n",
    "\n",
    "//=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the data directly from github\n",
    "haber = 'http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data'\n",
    "\n",
    "# Data does not have a header row so we have to label the data\n",
    "names = ['age', 'year', 'nodes', 'survival']\n",
    "data = pd.read_csv(haber, header=None, names=names)\n",
    "\n",
    "# We will change the survival labels\n",
    "# 1-> 1 : No change\n",
    "# 2-> 0 : Death within 5 years is 0\n",
    "data['survival']=-1*(data['survival']-2)\n",
    "\n",
    "# head() gives a snapshot of the data.  Jupyterhub is great a rendering tables.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'survival'].values\n",
    "y = data.loc[:, data.columns == 'survival'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Calculation\n",
    "\n",
    "To evaluate the learning of the parameters, we need an error equation. In linear regression, the error was the sum-squared error. For logistic regression, we will use the the sum of absolute error since the error is -1, 0, or 1 because of the binary output condition.\n",
    "\n",
    "In this case, $\\hat y_i$ is the calculated output over all data indicated by $i$.\n",
    "\n",
    "$$\n",
    "E = \\sum_i |y_i - \\hat y_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "We cannot solve for the parameters, $\\beta$, like we did for linear regression. Therefore we have to find the parameters through an iterative process.\n",
    "\n",
    "One such process is stochastic gradient descent. In this process, the parameters are updated with each new data point. The updates are done such that the parameters move in the direction of greatest decrease in the error.\n",
    "\n",
    "## Parameter Updating\n",
    "\n",
    "The equation to update each parameter, $\\beta_n$, uses the current value and a learning factor, $\\eta$.\n",
    "\n",
    "$$\n",
    "\\beta_{n,\\textrm{new}} = \\beta_{n,\\textrm{old}} + \\eta (y_i - \\hat y_i) x_{n,i}\n",
    "$$\n",
    "\n",
    "So, for our 3 inputs the following update equations are used after each new data record is read.\n",
    "\n",
    "$$\n",
    "\\beta_{0,\\textrm{new}} = \\beta_{0,\\textrm{old}} + \\eta (y_i - \\hat y_i) \\\\\n",
    "\\beta_{1,\\textrm{new}} = \\beta_{1,\\textrm{old}} + \\eta (y_i - \\hat y_i) x_{1,i}\\\\\n",
    "\\beta_{2,\\textrm{new}} = \\beta_{2,\\textrm{old}} + \\eta (y_i - \\hat y_i) x_{2,i}\\\\\n",
    "\\beta_{3,\\textrm{new}} = \\beta_{3,\\textrm{old}} + \\eta (y_i - \\hat y_i) x_{3,i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the Algorithm Together\n",
    "\n",
    "The algorithm is put together in this order:\n",
    "\n",
    "1. Create training data and test data from the available dataset\n",
    "2. Make initial guesses for the equation parameters\n",
    "3. Loop through the data\n",
    "    1. Calculate the predicted output\n",
    "    2. Update the parameters\n",
    "4. Calculate error on test data\n",
    "5. Repeat steps 3 and 4 multiple times (each pass through the data is called an Epoch)\n",
    "6. Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Sets\n",
    "\n",
    "The input data needs to be randomly assigned to either the training data or the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Prediction\n",
    "\n",
    "Here we will calculate the predicted output using a function. This function can be included in the loop. The predicted output can be between 0 and 1 so we will round the output so that our prediction is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_survival( beta, inputs):\n",
    "    t = beta[0] + beta[1]*inputs[0] + beta[2]*inputs[1] + beta[3]*inputs[2]\n",
    "    \n",
    "    y_ = 1/(1+np.exp(-1*t))\n",
    "    \n",
    "    assert y_ >= 0.0, \"Prediction is less than zero\"\n",
    "    assert y_ <= 1.0, \"Prediction is greater than one\"\n",
    "    \n",
    "    return ______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "\n",
    "The parameter updates require the prediction, the actual output, the parameters and the inputs. Again, this is functionized for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params( beta, predict, actual, inputs, eta=0.2):\n",
    "    delta = actual - predict\n",
    "    \n",
    "    beta[0] = beta[0] + eta*delta\n",
    "    beta[1] = beta[1] + eta*delta*inputs[0]\n",
    "    beta[2] = beta[2] + eta*delta*inputs[1]\n",
    "    beta[3] = beta[3] + eta*delta*inputs[2]\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Calculation\n",
    "\n",
    "The error calculation is the sum of the absolute error so it is easy to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_calc( actual, predict):\n",
    "    '''\n",
    "    Here the actual and predict inputs are assumed to be arrays.\n",
    "    '''\n",
    "    delta = actual - predict\n",
    "    \n",
    "    return np.sum(np.abs(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Over Epochs\n",
    "\n",
    "Now that the functions are created and the data is ready, we can write a loop to iterate over the data and over the epochs and print the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepoch = ______\n",
    "eta = ______\n",
    "N = len(y_train)\n",
    "\n",
    "beta = np.array([0, 55, 60, 1])\n",
    "\n",
    "error_array = np.zeros(nepoch)\n",
    "\n",
    "for n in range(nepoch):\n",
    "    y_train_predict = np.zeros_like(y_train)\n",
    "    y_test_predict = np.zeros_like(y_test)\n",
    "       \n",
    "    for i in range(N):\n",
    "        inputs = X_train[i,:]\n",
    "        y_train_predict[i] = calc_survival(beta, inputs)\n",
    "        beta = update_params(beta, y_train_predict[i], y_train[i], inputs, eta=eta)\n",
    "        \n",
    "    for j in range(len(y_test)):\n",
    "        y_test_predict[j] = calc_survival(beta, X_test[j,:])\n",
    "    \n",
    "    print(beta)\n",
    "    error_array[n] = error_calc(y_test, y_test_predict)\n",
    "    \n",
    "error_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
